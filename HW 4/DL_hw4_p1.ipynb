{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5fId-6VOl2_"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "# import tensorflow.keras as keras\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "referenced_widgets": [
      "9373f1b3659a49f79d1de6949ac01105",
      "bdcd2bd4c9224325819a3d22aa27adf8",
      "929dde6e586c4c96996398be4261549a",
      "f066ba75d2874803a77a9b4885d0d33c",
      "06d2e04735464f83b6f8701b70eb59e4",
      "27c4809878264ddca62e70e6acf4e907",
      "357252f5ea7a489dbbb6cd1439bf7412",
      "ed442ba6c9414e66abb543363152528f",
      "f67eaa867b584de79feb8f272dcacbd2",
      "13b5eeb83e104c86bc30b189bb62b126",
      "2791c6950e8a46aaab67f6a29852a15b"
     ]
    },
    "id": "M-0vV8yfOpx1",
    "outputId": "8a20136f-2735-464d-fdf8-1b72f8ac5ba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        # transforms.RandomResizedCrop(224),\n",
    "        # transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        # transforms.Resize(256),\n",
    "        # transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "image_datasets = {'train': datasets.CIFAR100('data',train=True,\n",
    "                                          transform=data_transforms['train'],download=True)\n",
    "                  ,'val': datasets.CIFAR100('data',train=False,\n",
    "                                          transform=data_transforms['val'],download=True)}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64,\n",
    "                                             shuffle=True)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baxKL8Kp1DG7"
   },
   "source": [
    "CIFAR-100 Dataset: \n",
    "\n",
    "No of classes = 100 \\\\\n",
    "No of images from each class = 600 \\\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "C8yRdY_0TINm",
    "outputId": "0236ba06-ec81-471f-c92d-6daeba06eb91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aquarium_fish\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottle\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782 157\n"
     ]
    }
   ],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) \n",
    "\n",
    "\n",
    "appind=[]\n",
    "sqind = []\n",
    "while(len(appind)<2 or len(sqind)<2):\n",
    "    inputs, classes = next(iter(dataloaders['train']))\n",
    "    for i in range(len(classes)):\n",
    "        if(len(appind)==2 and len(sqind)==2):\n",
    "            break\n",
    "        if(classes[i]==1 and len(appind)<2):\n",
    "            appind.append(inputs[i])\n",
    "        if(classes[i]==9 and len(sqind)<2):\n",
    "            sqind.append(inputs[i])\n",
    "print(class_names[1])\n",
    "imshow(appind[0])\n",
    "imshow(appind[1])\n",
    "plt.show()\n",
    "print(class_names[9])\n",
    "imshow(sqind[0])\n",
    "plt.show()\n",
    "imshow(sqind[1])\n",
    "plt.show()\n",
    "print(len(dataloaders['train']),len(dataloaders['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4PN6mUmTKLv"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    acc = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        # print('-' * 10)\n",
    "        epoch_time = time.time()\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train','val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            acc.append(epoch_acc.item())\n",
    "#             print(acc)\n",
    "            # if(epoch>10):\n",
    "            #     if(np.mean(acc[-10:])>epoch_acc):\n",
    "            #         break\n",
    "            \n",
    "        # if(epoch>10):\n",
    "        #         if(np.mean(acc[-10:])>epoch_acc):\n",
    "        #             break\n",
    "        print('Epoch time ',time.time()-epoch_time)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_jQtXQ0TNPl"
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mj121JqxTRad"
   },
   "outputs": [],
   "source": [
    "# 1.b\n",
    "model_ft = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 100)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3Cm5eH04whf",
    "outputId": "732efa7d-bd89-4d57-dfff-c6b352b03a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.8499 Acc: 0.3109\n",
      "val Loss: 1.9258 Acc: 0.4776\n",
      "Epoch time  55.763360023498535\n",
      "\n",
      "train Loss: 1.6736 Acc: 0.5368\n",
      "val Loss: 1.6445 Acc: 0.5394\n",
      "Epoch time  57.77497148513794\n",
      "\n",
      "train Loss: 1.2350 Acc: 0.6481\n",
      "val Loss: 1.5507 Acc: 0.5733\n",
      "Epoch time  58.09333062171936\n",
      "\n",
      "train Loss: 0.9281 Acc: 0.7276\n",
      "val Loss: 1.5609 Acc: 0.5766\n",
      "Epoch time  58.177814960479736\n",
      "\n",
      "train Loss: 0.6928 Acc: 0.7936\n",
      "val Loss: 1.6388 Acc: 0.5824\n",
      "Epoch time  57.26201128959656\n",
      "\n",
      "train Loss: 0.5224 Acc: 0.8419\n",
      "val Loss: 1.6440 Acc: 0.5908\n",
      "Epoch time  58.15801215171814\n",
      "\n",
      "train Loss: 0.3903 Acc: 0.8827\n",
      "val Loss: 1.7283 Acc: 0.5881\n",
      "Epoch time  59.35018253326416\n",
      "\n",
      "train Loss: 0.2941 Acc: 0.9104\n",
      "val Loss: 1.7839 Acc: 0.5945\n",
      "Epoch time  58.512551069259644\n",
      "\n",
      "train Loss: 0.2341 Acc: 0.9298\n",
      "val Loss: 1.8884 Acc: 0.5842\n",
      "Epoch time  58.34802508354187\n",
      "\n",
      "train Loss: 0.1951 Acc: 0.9412\n",
      "val Loss: 1.8662 Acc: 0.5957\n",
      "Epoch time  58.233871936798096\n",
      "\n",
      "train Loss: 0.1670 Acc: 0.9500\n",
      "val Loss: 1.9103 Acc: 0.5982\n",
      "Epoch time  58.50404477119446\n",
      "\n",
      "train Loss: 0.1301 Acc: 0.9613\n",
      "val Loss: 1.9514 Acc: 0.5944\n",
      "Epoch time  57.047507524490356\n",
      "\n",
      "train Loss: 0.1153 Acc: 0.9658\n",
      "val Loss: 1.9796 Acc: 0.6012\n",
      "Epoch time  57.02501702308655\n",
      "\n",
      "train Loss: 0.0951 Acc: 0.9720\n",
      "val Loss: 2.0204 Acc: 0.5990\n",
      "Epoch time  57.272584438323975\n",
      "\n",
      "train Loss: 0.0932 Acc: 0.9724\n",
      "val Loss: 2.0502 Acc: 0.5947\n",
      "Epoch time  57.72999143600464\n",
      "\n",
      "train Loss: 0.0753 Acc: 0.9784\n",
      "val Loss: 2.0841 Acc: 0.5974\n",
      "Epoch time  57.66560935974121\n",
      "\n",
      "train Loss: 0.0674 Acc: 0.9806\n",
      "val Loss: 2.0854 Acc: 0.6018\n",
      "Epoch time  58.29571294784546\n",
      "\n",
      "train Loss: 0.0660 Acc: 0.9805\n",
      "val Loss: 2.0924 Acc: 0.6036\n",
      "Epoch time  56.975600242614746\n",
      "\n",
      "train Loss: 0.0600 Acc: 0.9824\n",
      "val Loss: 2.1164 Acc: 0.6083\n",
      "Epoch time  55.444825649261475\n",
      "\n",
      "train Loss: 0.0562 Acc: 0.9835\n",
      "val Loss: 2.1313 Acc: 0.6021\n",
      "Epoch time  55.32250499725342\n",
      "\n",
      "train Loss: 0.0354 Acc: 0.9903\n",
      "val Loss: 2.0566 Acc: 0.6100\n",
      "Epoch time  58.04593205451965\n",
      "\n",
      "train Loss: 0.0237 Acc: 0.9944\n",
      "val Loss: 2.0234 Acc: 0.6120\n",
      "Epoch time  58.01337242126465\n",
      "\n",
      "train Loss: 0.0200 Acc: 0.9952\n",
      "val Loss: 2.0356 Acc: 0.6139\n",
      "Epoch time  57.7941312789917\n",
      "\n",
      "train Loss: 0.0158 Acc: 0.9968\n",
      "val Loss: 2.0287 Acc: 0.6158\n",
      "Epoch time  57.50680184364319\n",
      "\n",
      "train Loss: 0.0145 Acc: 0.9972\n",
      "val Loss: 2.0270 Acc: 0.6172\n",
      "Epoch time  55.78487014770508\n",
      "\n",
      "train Loss: 0.0126 Acc: 0.9978\n",
      "val Loss: 2.0140 Acc: 0.6179\n",
      "Epoch time  55.38099002838135\n",
      "\n",
      "train Loss: 0.0121 Acc: 0.9978\n",
      "val Loss: 2.0249 Acc: 0.6195\n",
      "Epoch time  55.64747095108032\n",
      "\n",
      "train Loss: 0.0122 Acc: 0.9975\n",
      "val Loss: 2.0075 Acc: 0.6176\n",
      "Epoch time  56.15922546386719\n",
      "\n",
      "train Loss: 0.0101 Acc: 0.9983\n",
      "val Loss: 2.0307 Acc: 0.6180\n",
      "Epoch time  56.95692276954651\n",
      "\n",
      "train Loss: 0.0102 Acc: 0.9980\n",
      "val Loss: 2.0239 Acc: 0.6148\n",
      "Epoch time  57.59196925163269\n",
      "\n",
      "train Loss: 0.0097 Acc: 0.9982\n",
      "val Loss: 2.0259 Acc: 0.6176\n",
      "Epoch time  57.89977550506592\n",
      "\n",
      "train Loss: 0.0095 Acc: 0.9985\n",
      "val Loss: 2.0329 Acc: 0.6200\n",
      "Epoch time  57.885183811187744\n",
      "\n",
      "train Loss: 0.0093 Acc: 0.9984\n",
      "val Loss: 2.0411 Acc: 0.6182\n",
      "Epoch time  57.90775418281555\n",
      "\n",
      "train Loss: 0.0090 Acc: 0.9983\n",
      "val Loss: 2.0379 Acc: 0.6181\n",
      "Epoch time  57.90753483772278\n",
      "\n",
      "train Loss: 0.0084 Acc: 0.9985\n",
      "val Loss: 2.0295 Acc: 0.6203\n",
      "Epoch time  57.0074725151062\n",
      "\n",
      "train Loss: 0.0082 Acc: 0.9986\n",
      "val Loss: 2.0316 Acc: 0.6198\n",
      "Epoch time  56.6029531955719\n",
      "\n",
      "train Loss: 0.0077 Acc: 0.9989\n",
      "val Loss: 2.0360 Acc: 0.6189\n",
      "Epoch time  56.77524185180664\n",
      "\n",
      "train Loss: 0.0074 Acc: 0.9989\n",
      "val Loss: 2.0226 Acc: 0.6211\n",
      "Epoch time  57.35583233833313\n",
      "\n",
      "train Loss: 0.0077 Acc: 0.9986\n",
      "val Loss: 2.0207 Acc: 0.6229\n",
      "Epoch time  58.264732360839844\n",
      "\n",
      "train Loss: 0.0070 Acc: 0.9989\n",
      "val Loss: 2.0346 Acc: 0.6205\n",
      "Epoch time  58.36427307128906\n",
      "\n",
      "train Loss: 0.0072 Acc: 0.9990\n",
      "val Loss: 2.0390 Acc: 0.6210\n",
      "Epoch time  58.34641170501709\n",
      "\n",
      "train Loss: 0.0070 Acc: 0.9992\n",
      "val Loss: 2.0354 Acc: 0.6230\n",
      "Epoch time  58.91853070259094\n",
      "\n",
      "train Loss: 0.0068 Acc: 0.9989\n",
      "val Loss: 2.0395 Acc: 0.6190\n",
      "Epoch time  58.47161364555359\n",
      "\n",
      "train Loss: 0.0068 Acc: 0.9990\n",
      "val Loss: 2.0286 Acc: 0.6231\n",
      "Epoch time  57.558327198028564\n",
      "\n",
      "train Loss: 0.0070 Acc: 0.9990\n",
      "val Loss: 2.0595 Acc: 0.6180\n",
      "Epoch time  58.49300003051758\n",
      "\n",
      "train Loss: 0.0067 Acc: 0.9988\n",
      "val Loss: 2.0380 Acc: 0.6200\n",
      "Epoch time  58.25450611114502\n",
      "\n",
      "train Loss: 0.0065 Acc: 0.9992\n",
      "val Loss: 2.0204 Acc: 0.6212\n",
      "Epoch time  57.82011079788208\n",
      "\n",
      "train Loss: 0.0069 Acc: 0.9987\n",
      "val Loss: 2.0365 Acc: 0.6227\n",
      "Epoch time  57.717150926589966\n",
      "\n",
      "train Loss: 0.0064 Acc: 0.9992\n",
      "val Loss: 2.0369 Acc: 0.6212\n",
      "Epoch time  55.51311373710632\n",
      "\n",
      "train Loss: 0.0065 Acc: 0.9990\n",
      "val Loss: 2.0355 Acc: 0.6223\n",
      "Epoch time  55.517115116119385\n",
      "\n",
      "train Loss: 0.0066 Acc: 0.9988\n",
      "val Loss: 2.0239 Acc: 0.6201\n",
      "Epoch time  55.80860495567322\n",
      "\n",
      "train Loss: 0.0067 Acc: 0.9987\n",
      "val Loss: 2.0351 Acc: 0.6195\n",
      "Epoch time  58.19627547264099\n",
      "\n",
      "train Loss: 0.0064 Acc: 0.9990\n",
      "val Loss: 2.0378 Acc: 0.6209\n",
      "Epoch time  57.911778688430786\n",
      "\n",
      "train Loss: 0.0066 Acc: 0.9990\n",
      "val Loss: 2.0499 Acc: 0.6212\n",
      "Epoch time  57.6910924911499\n",
      "\n",
      "train Loss: 0.0064 Acc: 0.9990\n",
      "val Loss: 2.0383 Acc: 0.6190\n",
      "Epoch time  56.91341590881348\n",
      "\n",
      "train Loss: 0.0060 Acc: 0.9992\n",
      "val Loss: 2.0491 Acc: 0.6211\n",
      "Epoch time  57.00497651100159\n",
      "\n",
      "train Loss: 0.0068 Acc: 0.9988\n",
      "val Loss: 2.0391 Acc: 0.6203\n",
      "Epoch time  57.220194578170776\n",
      "\n",
      "train Loss: 0.0063 Acc: 0.9990\n",
      "val Loss: 2.0395 Acc: 0.6198\n",
      "Epoch time  56.7869770526886\n",
      "\n",
      "train Loss: 0.0062 Acc: 0.9992\n",
      "val Loss: 2.0351 Acc: 0.6205\n",
      "Epoch time  57.028815031051636\n",
      "\n",
      "train Loss: 0.0064 Acc: 0.9990\n",
      "val Loss: 2.0352 Acc: 0.6203\n",
      "Epoch time  57.3436074256897\n",
      "\n",
      "Training complete in 57m 24s\n",
      "Best val Acc: 0.623100\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvEyU1T84wsk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2Ax3WQx4w2y"
   },
   "outputs": [],
   "source": [
    "# 1.c\n",
    "model_ft = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "model_ft.fc = nn.Linear(num_ftrs, 100)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Choosing large step_size so that the learning rate is not changed\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10000, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rnEzLdC4zyf",
    "outputId": "0adac28e-ca43-4a2e-9ac5-63b69615377a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2.7712 Acc: 0.3057\n",
      "val Loss: 2.6675 Acc: 0.3508\n",
      "Epoch time  58.030938148498535\n",
      "\n",
      "train Loss: 2.1802 Acc: 0.4218\n",
      "val Loss: 2.0859 Acc: 0.4774\n",
      "Epoch time  57.85194158554077\n",
      "\n",
      "train Loss: 1.7993 Acc: 0.5077\n",
      "val Loss: 2.0647 Acc: 0.4725\n",
      "Epoch time  58.15672492980957\n",
      "\n",
      "train Loss: 1.5521 Acc: 0.5681\n",
      "val Loss: 2.3339 Acc: 0.4071\n",
      "Epoch time  57.89279866218567\n",
      "\n",
      "train Loss: 1.5433 Acc: 0.5655\n",
      "val Loss: 1.9238 Acc: 0.5026\n",
      "Epoch time  57.6814501285553\n",
      "\n",
      "train Loss: 1.1544 Acc: 0.6612\n",
      "val Loss: 1.7459 Acc: 0.5435\n",
      "Epoch time  57.32026720046997\n",
      "\n",
      "train Loss: 0.9221 Acc: 0.7247\n",
      "val Loss: 1.8882 Acc: 0.5311\n",
      "Epoch time  57.45919370651245\n",
      "\n",
      "train Loss: 0.9180 Acc: 0.7238\n",
      "val Loss: 1.8197 Acc: 0.5517\n",
      "Epoch time  57.603549003601074\n",
      "\n",
      "train Loss: 0.6886 Acc: 0.7892\n",
      "val Loss: 1.9484 Acc: 0.5411\n",
      "Epoch time  56.4409384727478\n",
      "\n",
      "train Loss: 0.4996 Acc: 0.8428\n",
      "val Loss: 2.0186 Acc: 0.5525\n",
      "Epoch time  55.63773560523987\n",
      "\n",
      "train Loss: 0.7799 Acc: 0.7684\n",
      "val Loss: 2.1421 Acc: 0.5356\n",
      "Epoch time  55.421449422836304\n",
      "\n",
      "train Loss: 0.5564 Acc: 0.8271\n",
      "val Loss: 2.3918 Acc: 0.5374\n",
      "Epoch time  55.504910945892334\n",
      "\n",
      "train Loss: 0.3830 Acc: 0.8789\n",
      "val Loss: 2.6917 Acc: 0.5315\n",
      "Epoch time  55.82636117935181\n",
      "\n",
      "train Loss: 0.2289 Acc: 0.9292\n",
      "val Loss: 2.4688 Acc: 0.5485\n",
      "Epoch time  57.35252809524536\n",
      "\n",
      "train Loss: 0.1485 Acc: 0.9550\n",
      "val Loss: 2.7103 Acc: 0.5511\n",
      "Epoch time  58.84634494781494\n",
      "\n",
      "train Loss: 0.1127 Acc: 0.9652\n",
      "val Loss: 2.7558 Acc: 0.5481\n",
      "Epoch time  56.59234261512756\n",
      "\n",
      "train Loss: 0.1884 Acc: 0.9419\n",
      "val Loss: 2.4895 Acc: 0.5475\n",
      "Epoch time  55.62031054496765\n",
      "\n",
      "train Loss: 0.0863 Acc: 0.9747\n",
      "val Loss: 2.6388 Acc: 0.5576\n",
      "Epoch time  55.47700333595276\n",
      "\n",
      "train Loss: 0.0726 Acc: 0.9784\n",
      "val Loss: 2.6401 Acc: 0.5585\n",
      "Epoch time  55.89385724067688\n",
      "\n",
      "train Loss: 0.0493 Acc: 0.9860\n",
      "val Loss: 2.7733 Acc: 0.5583\n",
      "Epoch time  55.55033779144287\n",
      "\n",
      "train Loss: 0.0417 Acc: 0.9884\n",
      "val Loss: 2.6932 Acc: 0.5672\n",
      "Epoch time  55.6079797744751\n",
      "\n",
      "train Loss: 0.0289 Acc: 0.9923\n",
      "val Loss: 2.8412 Acc: 0.5629\n",
      "Epoch time  57.501354932785034\n",
      "\n",
      "train Loss: 0.0260 Acc: 0.9929\n",
      "val Loss: 2.8409 Acc: 0.5565\n",
      "Epoch time  58.0450701713562\n",
      "\n",
      "train Loss: 0.0240 Acc: 0.9934\n",
      "val Loss: 2.9229 Acc: 0.5654\n",
      "Epoch time  57.62529492378235\n",
      "\n",
      "train Loss: 0.0295 Acc: 0.9919\n",
      "val Loss: 2.8075 Acc: 0.5727\n",
      "Epoch time  57.70913362503052\n",
      "\n",
      "train Loss: 0.0202 Acc: 0.9947\n",
      "val Loss: 3.1831 Acc: 0.5659\n",
      "Epoch time  58.32771182060242\n",
      "\n",
      "train Loss: 0.0197 Acc: 0.9948\n",
      "val Loss: 3.1637 Acc: 0.5616\n",
      "Epoch time  58.042157888412476\n",
      "\n",
      "train Loss: 0.0237 Acc: 0.9934\n",
      "val Loss: 3.2636 Acc: 0.5567\n",
      "Epoch time  58.14348387718201\n",
      "\n",
      "train Loss: 0.1561 Acc: 0.9518\n",
      "val Loss: 2.8239 Acc: 0.5426\n",
      "Epoch time  58.16214203834534\n",
      "\n",
      "train Loss: 0.1942 Acc: 0.9408\n",
      "val Loss: 2.4554 Acc: 0.5577\n",
      "Epoch time  58.448967695236206\n",
      "\n",
      "train Loss: 0.0421 Acc: 0.9882\n",
      "val Loss: 2.5188 Acc: 0.5703\n",
      "Epoch time  58.26853036880493\n",
      "\n",
      "train Loss: 0.0258 Acc: 0.9932\n",
      "val Loss: 2.6841 Acc: 0.5629\n",
      "Epoch time  57.95757460594177\n",
      "\n",
      "train Loss: 0.0224 Acc: 0.9938\n",
      "val Loss: 2.6702 Acc: 0.5683\n",
      "Epoch time  57.83512616157532\n",
      "\n",
      "train Loss: 0.0168 Acc: 0.9956\n",
      "val Loss: 2.7818 Acc: 0.5748\n",
      "Epoch time  59.14642286300659\n",
      "\n",
      "train Loss: 0.0199 Acc: 0.9946\n",
      "val Loss: 2.7068 Acc: 0.5747\n",
      "Epoch time  58.387003898620605\n",
      "\n",
      "train Loss: 0.0115 Acc: 0.9970\n",
      "val Loss: 2.8162 Acc: 0.5723\n",
      "Epoch time  58.876683712005615\n",
      "\n",
      "train Loss: 0.0239 Acc: 0.9930\n",
      "val Loss: 2.7291 Acc: 0.5713\n",
      "Epoch time  58.38813591003418\n",
      "\n",
      "train Loss: 0.0143 Acc: 0.9960\n",
      "val Loss: 2.8142 Acc: 0.5661\n",
      "Epoch time  58.65033793449402\n",
      "\n",
      "train Loss: 0.0140 Acc: 0.9963\n",
      "val Loss: 2.7849 Acc: 0.5698\n",
      "Epoch time  59.243545055389404\n",
      "\n",
      "train Loss: 0.0176 Acc: 0.9948\n",
      "val Loss: 2.7767 Acc: 0.5720\n",
      "Epoch time  58.97233533859253\n",
      "\n",
      "train Loss: 0.0136 Acc: 0.9962\n",
      "val Loss: 2.8170 Acc: 0.5680\n",
      "Epoch time  58.999650716781616\n",
      "\n",
      "train Loss: 0.0097 Acc: 0.9973\n",
      "val Loss: 2.8499 Acc: 0.5728\n",
      "Epoch time  59.15577292442322\n",
      "\n",
      "train Loss: 0.0111 Acc: 0.9969\n",
      "val Loss: 2.9500 Acc: 0.5677\n",
      "Epoch time  58.34913182258606\n",
      "\n",
      "train Loss: 0.0102 Acc: 0.9968\n",
      "val Loss: 2.9378 Acc: 0.5674\n",
      "Epoch time  58.54762268066406\n",
      "\n",
      "train Loss: 0.0059 Acc: 0.9986\n",
      "val Loss: 2.8747 Acc: 0.5826\n",
      "Epoch time  58.596367597579956\n",
      "\n",
      "train Loss: 0.0064 Acc: 0.9983\n",
      "val Loss: 2.8930 Acc: 0.5801\n",
      "Epoch time  57.54751014709473\n",
      "\n",
      "train Loss: 0.0054 Acc: 0.9985\n",
      "val Loss: 3.0666 Acc: 0.5672\n",
      "Epoch time  56.887218713760376\n",
      "\n",
      "train Loss: 0.0052 Acc: 0.9985\n",
      "val Loss: 2.9798 Acc: 0.5787\n",
      "Epoch time  56.546945571899414\n",
      "\n",
      "train Loss: 0.0042 Acc: 0.9990\n",
      "val Loss: 2.9950 Acc: 0.5794\n",
      "Epoch time  57.32757496833801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1vy-ayqCViQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEPC0OnoCVwt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TW4QmZ-2CWCW"
   },
   "outputs": [],
   "source": [
    "model_ft = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 100)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=10000, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vQPywFOCX9T",
    "outputId": "61eea225-84b7-464d-d284-541f5d349e4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 4.2924 Acc: 0.0569\n",
      "val Loss: 3.8239 Acc: 0.1034\n",
      "Epoch time  31.980154037475586\n",
      "\n",
      "train Loss: 3.6444 Acc: 0.1322\n",
      "val Loss: 3.6808 Acc: 0.1663\n",
      "Epoch time  31.474239349365234\n",
      "\n",
      "train Loss: 3.3573 Acc: 0.1850\n",
      "val Loss: 168.0836 Acc: 0.0916\n",
      "Epoch time  31.48808240890503\n",
      "\n",
      "train Loss: 3.4314 Acc: 0.1724\n",
      "val Loss: 3.3322 Acc: 0.2005\n",
      "Epoch time  30.927753925323486\n",
      "\n",
      "train Loss: 3.1328 Acc: 0.2295\n",
      "val Loss: 3.0938 Acc: 0.2479\n",
      "Epoch time  31.615440130233765\n",
      "\n",
      "train Loss: 2.9113 Acc: 0.2709\n",
      "val Loss: 2.9317 Acc: 0.2751\n",
      "Epoch time  31.68801212310791\n",
      "\n",
      "train Loss: 2.7537 Acc: 0.3027\n",
      "val Loss: 2.8398 Acc: 0.2987\n",
      "Epoch time  31.393277406692505\n",
      "\n",
      "train Loss: 2.5957 Acc: 0.3343\n",
      "val Loss: 2.7763 Acc: 0.3128\n",
      "Epoch time  31.085713148117065\n",
      "\n",
      "train Loss: 2.4539 Acc: 0.3604\n",
      "val Loss: 2.7110 Acc: 0.3291\n",
      "Epoch time  31.286401510238647\n",
      "\n",
      "train Loss: 2.2735 Acc: 0.3977\n",
      "val Loss: 2.6343 Acc: 0.3375\n",
      "Epoch time  31.146859645843506\n",
      "\n",
      "train Loss: 2.1366 Acc: 0.4286\n",
      "val Loss: 2.8236 Acc: 0.3339\n",
      "Epoch time  31.259514331817627\n",
      "\n",
      "train Loss: 1.9864 Acc: 0.4626\n",
      "val Loss: 2.7250 Acc: 0.3452\n",
      "Epoch time  31.060490369796753\n",
      "\n",
      "train Loss: 1.8352 Acc: 0.4921\n",
      "val Loss: 2.7634 Acc: 0.3495\n",
      "Epoch time  31.007502555847168\n",
      "\n",
      "train Loss: 1.6819 Acc: 0.5293\n",
      "val Loss: 2.7838 Acc: 0.3488\n",
      "Epoch time  31.054474592208862\n",
      "\n",
      "train Loss: 1.5196 Acc: 0.5681\n",
      "val Loss: 2.8369 Acc: 0.3598\n",
      "Epoch time  30.796119213104248\n",
      "\n",
      "train Loss: 1.3594 Acc: 0.6057\n",
      "val Loss: 2.9191 Acc: 0.3626\n",
      "Epoch time  30.9757080078125\n",
      "\n",
      "train Loss: 1.1989 Acc: 0.6432\n",
      "val Loss: 3.0496 Acc: 0.3616\n",
      "Epoch time  30.57920002937317\n",
      "\n",
      "train Loss: 1.0573 Acc: 0.6801\n",
      "val Loss: 3.1772 Acc: 0.3610\n",
      "Epoch time  30.768463373184204\n",
      "\n",
      "train Loss: 0.9159 Acc: 0.7190\n",
      "val Loss: 3.2880 Acc: 0.3570\n",
      "Epoch time  31.05414843559265\n",
      "\n",
      "train Loss: 0.7873 Acc: 0.7554\n",
      "val Loss: 3.4781 Acc: 0.3575\n",
      "Epoch time  30.541253805160522\n",
      "\n",
      "train Loss: 0.6686 Acc: 0.7913\n",
      "val Loss: 3.6698 Acc: 0.3589\n",
      "Epoch time  30.874783754348755\n",
      "\n",
      "train Loss: 0.5819 Acc: 0.8134\n",
      "val Loss: 3.8899 Acc: 0.3591\n",
      "Epoch time  30.43886709213257\n",
      "\n",
      "train Loss: 0.5042 Acc: 0.8375\n",
      "val Loss: 3.9764 Acc: 0.3571\n",
      "Epoch time  30.431280612945557\n",
      "\n",
      "train Loss: 0.4322 Acc: 0.8609\n",
      "val Loss: 4.1886 Acc: 0.3566\n",
      "Epoch time  30.821301221847534\n",
      "\n",
      "train Loss: 0.3973 Acc: 0.8709\n",
      "val Loss: 4.2789 Acc: 0.3567\n",
      "Epoch time  30.96369433403015\n",
      "\n",
      "train Loss: 0.3307 Acc: 0.8933\n",
      "val Loss: 4.4170 Acc: 0.3658\n",
      "Epoch time  30.863706588745117\n",
      "\n",
      "train Loss: 0.3077 Acc: 0.8991\n",
      "val Loss: 4.4953 Acc: 0.3677\n",
      "Epoch time  31.13179922103882\n",
      "\n",
      "train Loss: 0.2717 Acc: 0.9125\n",
      "val Loss: 4.4545 Acc: 0.3732\n",
      "Epoch time  31.119428873062134\n",
      "\n",
      "train Loss: 0.2205 Acc: 0.9290\n",
      "val Loss: 4.7067 Acc: 0.3800\n",
      "Epoch time  31.181434631347656\n",
      "\n",
      "train Loss: 0.2106 Acc: 0.9308\n",
      "val Loss: 4.7914 Acc: 0.3704\n",
      "Epoch time  30.692995309829712\n",
      "\n",
      "train Loss: 0.2023 Acc: 0.9338\n",
      "val Loss: 4.9161 Acc: 0.3642\n",
      "Epoch time  31.233875274658203\n",
      "\n",
      "train Loss: 0.1718 Acc: 0.9445\n",
      "val Loss: 4.9637 Acc: 0.3781\n",
      "Epoch time  30.946407318115234\n",
      "\n",
      "train Loss: 0.1715 Acc: 0.9436\n",
      "val Loss: 5.0679 Acc: 0.3725\n",
      "Epoch time  31.108503818511963\n",
      "\n",
      "train Loss: 0.1573 Acc: 0.9494\n",
      "val Loss: 5.1142 Acc: 0.3726\n",
      "Epoch time  30.99872875213623\n",
      "\n",
      "train Loss: 0.1489 Acc: 0.9506\n",
      "val Loss: 5.1956 Acc: 0.3766\n",
      "Epoch time  31.3347806930542\n",
      "\n",
      "train Loss: 0.1276 Acc: 0.9590\n",
      "val Loss: 5.2247 Acc: 0.3719\n",
      "Epoch time  30.515453815460205\n",
      "\n",
      "train Loss: 0.1264 Acc: 0.9580\n",
      "val Loss: 5.2813 Acc: 0.3832\n",
      "Epoch time  30.650399923324585\n",
      "\n",
      "train Loss: 0.1141 Acc: 0.9632\n",
      "val Loss: 5.3056 Acc: 0.3797\n",
      "Epoch time  31.04868221282959\n",
      "\n",
      "train Loss: 0.1221 Acc: 0.9604\n",
      "val Loss: 5.3978 Acc: 0.3774\n",
      "Epoch time  31.3729350566864\n",
      "\n",
      "train Loss: 0.1102 Acc: 0.9643\n",
      "val Loss: 5.3753 Acc: 0.3869\n",
      "Epoch time  31.226417303085327\n",
      "\n",
      "train Loss: 0.0930 Acc: 0.9703\n",
      "val Loss: 5.4571 Acc: 0.3818\n",
      "Epoch time  31.541901111602783\n",
      "\n",
      "train Loss: 0.0895 Acc: 0.9710\n",
      "val Loss: 5.5206 Acc: 0.3878\n",
      "Epoch time  30.865559101104736\n",
      "\n",
      "train Loss: 0.0879 Acc: 0.9718\n",
      "val Loss: 5.6052 Acc: 0.3830\n",
      "Epoch time  30.800562620162964\n",
      "\n",
      "train Loss: 0.0763 Acc: 0.9756\n",
      "val Loss: 5.5876 Acc: 0.3861\n",
      "Epoch time  31.292702198028564\n",
      "\n",
      "train Loss: 0.0731 Acc: 0.9772\n",
      "val Loss: 5.6854 Acc: 0.3852\n",
      "Epoch time  31.08293080329895\n",
      "\n",
      "train Loss: 0.0780 Acc: 0.9748\n",
      "val Loss: 5.6647 Acc: 0.3831\n",
      "Epoch time  31.35322141647339\n",
      "\n",
      "train Loss: 0.0745 Acc: 0.9752\n",
      "val Loss: 5.7817 Acc: 0.3835\n",
      "Epoch time  30.57904863357544\n",
      "\n",
      "train Loss: 0.0691 Acc: 0.9780\n",
      "val Loss: 5.8785 Acc: 0.3758\n",
      "Epoch time  30.87091326713562\n",
      "\n",
      "train Loss: 0.0616 Acc: 0.9800\n",
      "val Loss: 5.7691 Acc: 0.3872\n",
      "Epoch time  30.638628244400024\n",
      "\n",
      "train Loss: 0.0663 Acc: 0.9787\n",
      "val Loss: 5.8226 Acc: 0.3886\n",
      "Epoch time  31.256396532058716\n",
      "\n",
      "Training complete in 25m 52s\n",
      "Best val Acc: 0.388600\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xv6mUuF7mbcT"
   },
   "source": [
    "All the three learning rates give very high training accuracies. \\\\\n",
    "The first learning rate used gives the highest validation accuracy on the target set. Validation acc = 0.6231 \\\\\n",
    "The second learning rate gives a lower accuracy. Validation acc = 0.5794 \\\\\n",
    "And the third onne gives the least among the three. Validation acc = 0.0.3886 \\\\\n",
    "Therefore the learning rate which changes from 0.0001 to 0.000001 gives the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9N0GSE4DEIym"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onGHFYjDEJH5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsOjx3dtEJXN"
   },
   "outputs": [],
   "source": [
    "# 2\n",
    "model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 100)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6vofYa7E5DM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XLtMRSiWESPx",
    "outputId": "32987cfb-7312-427d-c12c-115234d3dea9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 142.7233 Acc: 0.1478\n",
      "val Loss: 171.0052 Acc: 0.1797\n",
      "Epoch time  18.774144172668457\n",
      "\n",
      "train Loss: 141.3311 Acc: 0.1995\n",
      "val Loss: 151.4047 Acc: 0.2069\n",
      "Epoch time  19.278165340423584\n",
      "\n",
      "train Loss: 138.1913 Acc: 0.2194\n",
      "val Loss: 159.6138 Acc: 0.2148\n",
      "Epoch time  23.403469800949097\n",
      "\n",
      "train Loss: 136.6077 Acc: 0.2316\n",
      "val Loss: 171.3207 Acc: 0.2133\n",
      "Epoch time  19.571969509124756\n",
      "\n",
      "train Loss: 136.5718 Acc: 0.2446\n",
      "val Loss: 160.7145 Acc: 0.2236\n",
      "Epoch time  42.624640703201294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.a\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=1, momentum=0.9)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=10000, gamma=0.1)\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SPfZTZqHXinQ",
    "outputId": "f4f4b730-d797-41d1-8d4c-a60cd9776a79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 12.4193 Acc: 0.1522\n",
      "val Loss: 13.3354 Acc: 0.1967\n",
      "Epoch time  18.814987421035767\n",
      "\n",
      "train Loss: 12.9655 Acc: 0.2006\n",
      "val Loss: 13.8303 Acc: 0.2042\n",
      "Epoch time  19.121588945388794\n",
      "\n",
      "train Loss: 12.6137 Acc: 0.2269\n",
      "val Loss: 14.4234 Acc: 0.2195\n",
      "Epoch time  18.31628394126892\n",
      "\n",
      "train Loss: 12.5771 Acc: 0.2405\n",
      "val Loss: 14.9399 Acc: 0.2203\n",
      "Epoch time  18.625174283981323\n",
      "\n",
      "train Loss: 12.5397 Acc: 0.2493\n",
      "val Loss: 16.4353 Acc: 0.2211\n",
      "Epoch time  18.258662223815918\n",
      "\n",
      "train Loss: 12.3467 Acc: 0.2607\n",
      "val Loss: 14.9972 Acc: 0.2243\n",
      "Epoch time  18.06866192817688\n",
      "\n",
      "train Loss: 12.5186 Acc: 0.2616\n",
      "val Loss: 16.0285 Acc: 0.2395\n",
      "Epoch time  18.48176622390747\n",
      "\n",
      "train Loss: 12.3498 Acc: 0.2734\n",
      "val Loss: 15.5772 Acc: 0.2353\n",
      "Epoch time  18.45037579536438\n",
      "\n",
      "train Loss: 12.3422 Acc: 0.2770\n",
      "val Loss: 16.9008 Acc: 0.2216\n",
      "Epoch time  18.900713205337524\n",
      "\n",
      "train Loss: 12.3976 Acc: 0.2786\n",
      "val Loss: 14.9404 Acc: 0.2392\n",
      "Epoch time  18.39568567276001\n",
      "\n",
      "train Loss: 12.4143 Acc: 0.2840\n",
      "val Loss: 16.1833 Acc: 0.2364\n",
      "Epoch time  18.090779304504395\n",
      "\n",
      "train Loss: 12.3747 Acc: 0.2888\n",
      "val Loss: 17.0165 Acc: 0.2397\n",
      "Epoch time  18.764973878860474\n",
      "\n",
      "train Loss: 12.4319 Acc: 0.2914\n",
      "val Loss: 16.5511 Acc: 0.2472\n",
      "Epoch time  18.688733339309692\n",
      "\n",
      "train Loss: 12.4158 Acc: 0.2944\n",
      "val Loss: 16.2017 Acc: 0.2446\n",
      "Epoch time  18.282764434814453\n",
      "\n",
      "train Loss: 12.4594 Acc: 0.2981\n",
      "val Loss: 16.9095 Acc: 0.2414\n",
      "Epoch time  18.392266273498535\n",
      "\n",
      "train Loss: 12.3975 Acc: 0.3026\n",
      "val Loss: 15.9357 Acc: 0.2483\n",
      "Epoch time  18.6424503326416\n",
      "\n",
      "train Loss: 12.4959 Acc: 0.3040\n",
      "val Loss: 17.2596 Acc: 0.2509\n",
      "Epoch time  18.948688507080078\n",
      "\n",
      "train Loss: 12.4470 Acc: 0.3070\n",
      "val Loss: 17.3174 Acc: 0.2432\n",
      "Epoch time  18.238624095916748\n",
      "\n",
      "train Loss: 12.3239 Acc: 0.3108\n",
      "val Loss: 17.9972 Acc: 0.2364\n",
      "Epoch time  18.6834454536438\n",
      "\n",
      "train Loss: 12.3602 Acc: 0.3072\n",
      "val Loss: 18.9149 Acc: 0.2431\n",
      "Epoch time  18.631000995635986\n",
      "\n",
      "train Loss: 12.4038 Acc: 0.3147\n",
      "val Loss: 17.3401 Acc: 0.2366\n",
      "Epoch time  18.467307806015015\n",
      "\n",
      "train Loss: 12.3262 Acc: 0.3121\n",
      "val Loss: 16.6315 Acc: 0.2387\n",
      "Epoch time  18.521456003189087\n",
      "\n",
      "train Loss: 12.4321 Acc: 0.3113\n",
      "val Loss: 18.2245 Acc: 0.2392\n",
      "Epoch time  18.539164304733276\n",
      "\n",
      "train Loss: 12.4157 Acc: 0.3149\n",
      "val Loss: 17.4765 Acc: 0.2464\n",
      "Epoch time  18.905505180358887\n",
      "\n",
      "train Loss: 12.4573 Acc: 0.3159\n",
      "val Loss: 18.3937 Acc: 0.2498\n",
      "Epoch time  18.694188117980957\n",
      "\n",
      "train Loss: 12.4915 Acc: 0.3171\n",
      "val Loss: 17.9113 Acc: 0.2555\n",
      "Epoch time  18.860309839248657\n",
      "\n",
      "train Loss: 12.4101 Acc: 0.3256\n",
      "val Loss: 18.1544 Acc: 0.2427\n",
      "Epoch time  18.9043231010437\n",
      "\n",
      "train Loss: 12.4781 Acc: 0.3185\n",
      "val Loss: 17.2629 Acc: 0.2484\n",
      "Epoch time  18.390761137008667\n",
      "\n",
      "train Loss: 12.5484 Acc: 0.3166\n",
      "val Loss: 18.5367 Acc: 0.2474\n",
      "Epoch time  18.593971729278564\n",
      "\n",
      "train Loss: 12.6139 Acc: 0.3209\n",
      "val Loss: 19.1488 Acc: 0.2358\n",
      "Epoch time  18.39016056060791\n",
      "\n",
      "train Loss: 12.6165 Acc: 0.3252\n",
      "val Loss: 17.3590 Acc: 0.2406\n",
      "Epoch time  18.17740249633789\n",
      "\n",
      "train Loss: 12.4926 Acc: 0.3270\n",
      "val Loss: 17.3005 Acc: 0.2589\n",
      "Epoch time  18.97906184196472\n",
      "\n",
      "train Loss: 12.5018 Acc: 0.3246\n",
      "val Loss: 17.6323 Acc: 0.2583\n",
      "Epoch time  18.65861749649048\n",
      "\n",
      "train Loss: 12.5476 Acc: 0.3297\n",
      "val Loss: 18.9945 Acc: 0.2318\n",
      "Epoch time  18.522048473358154\n",
      "\n",
      "train Loss: 12.4767 Acc: 0.3307\n",
      "val Loss: 17.3303 Acc: 0.2580\n",
      "Epoch time  18.37920355796814\n",
      "\n",
      "train Loss: 12.4772 Acc: 0.3272\n",
      "val Loss: 17.9518 Acc: 0.2448\n",
      "Epoch time  18.11619281768799\n",
      "\n",
      "train Loss: 12.5231 Acc: 0.3277\n",
      "val Loss: 19.4950 Acc: 0.2433\n",
      "Epoch time  18.631069898605347\n",
      "\n",
      "train Loss: 12.5970 Acc: 0.3260\n",
      "val Loss: 19.3519 Acc: 0.2437\n",
      "Epoch time  18.338542938232422\n",
      "\n",
      "train Loss: 12.6481 Acc: 0.3292\n",
      "val Loss: 18.3148 Acc: 0.2434\n",
      "Epoch time  18.11570405960083\n",
      "\n",
      "train Loss: 12.5332 Acc: 0.3296\n",
      "val Loss: 19.0482 Acc: 0.2473\n",
      "Epoch time  17.68524694442749\n",
      "\n",
      "train Loss: 12.5692 Acc: 0.3296\n",
      "val Loss: 17.8610 Acc: 0.2541\n",
      "Epoch time  18.297410488128662\n",
      "\n",
      "train Loss: 12.4473 Acc: 0.3370\n",
      "val Loss: 20.5542 Acc: 0.2364\n",
      "Epoch time  18.84179997444153\n",
      "\n",
      "train Loss: 12.6199 Acc: 0.3337\n",
      "val Loss: 19.0842 Acc: 0.2489\n",
      "Epoch time  18.404136896133423\n",
      "\n",
      "train Loss: 12.7169 Acc: 0.3319\n",
      "val Loss: 16.9644 Acc: 0.2529\n",
      "Epoch time  18.529159545898438\n",
      "\n",
      "train Loss: 12.5589 Acc: 0.3348\n",
      "val Loss: 18.2878 Acc: 0.2461\n",
      "Epoch time  18.634140729904175\n",
      "\n",
      "train Loss: 12.7319 Acc: 0.3320\n",
      "val Loss: 20.5383 Acc: 0.2457\n",
      "Epoch time  18.29822039604187\n",
      "\n",
      "train Loss: 12.6630 Acc: 0.3357\n",
      "val Loss: 18.3201 Acc: 0.2546\n",
      "Epoch time  18.815609216690063\n",
      "\n",
      "train Loss: 12.6892 Acc: 0.3350\n",
      "val Loss: 21.1016 Acc: 0.2463\n",
      "Epoch time  18.087939977645874\n",
      "\n",
      "train Loss: 12.5158 Acc: 0.3388\n",
      "val Loss: 18.6747 Acc: 0.2505\n",
      "Epoch time  18.7412850856781\n",
      "\n",
      "train Loss: 12.7079 Acc: 0.3354\n",
      "val Loss: 19.0539 Acc: 0.2485\n",
      "Epoch time  17.92252206802368\n",
      "\n",
      "train Loss: 12.7228 Acc: 0.3324\n",
      "val Loss: 19.5185 Acc: 0.2551\n",
      "Epoch time  19.005841970443726\n",
      "\n",
      "train Loss: 12.7735 Acc: 0.3349\n",
      "val Loss: 19.0301 Acc: 0.2476\n",
      "Epoch time  18.631060361862183\n",
      "\n",
      "train Loss: 12.6297 Acc: 0.3368\n",
      "val Loss: 19.9145 Acc: 0.2454\n",
      "Epoch time  18.449285984039307\n",
      "\n",
      "train Loss: 12.7246 Acc: 0.3394\n",
      "val Loss: 20.0932 Acc: 0.2462\n",
      "Epoch time  18.259653329849243\n",
      "\n",
      "train Loss: 12.6624 Acc: 0.3388\n",
      "val Loss: 18.6864 Acc: 0.2513\n",
      "Epoch time  18.20927619934082\n",
      "\n",
      "train Loss: 12.7118 Acc: 0.3396\n",
      "val Loss: 18.6691 Acc: 0.2559\n",
      "Epoch time  18.777048110961914\n",
      "\n",
      "train Loss: 12.6673 Acc: 0.3353\n",
      "val Loss: 18.8280 Acc: 0.2550\n",
      "Epoch time  18.912349224090576\n",
      "\n",
      "train Loss: 12.6012 Acc: 0.3398\n",
      "val Loss: 18.9135 Acc: 0.2529\n",
      "Epoch time  18.48427438735962\n",
      "\n",
      "train Loss: 12.6879 Acc: 0.3412\n",
      "val Loss: 18.4303 Acc: 0.2525\n",
      "Epoch time  18.532434701919556\n",
      "\n",
      "train Loss: 12.7536 Acc: 0.3385\n",
      "val Loss: 19.2484 Acc: 0.2585\n",
      "Epoch time  18.67524766921997\n",
      "\n",
      "train Loss: 12.6814 Acc: 0.3442\n",
      "val Loss: 19.4728 Acc: 0.2433\n",
      "Epoch time  18.192166805267334\n",
      "\n",
      "train Loss: 12.7393 Acc: 0.3386\n",
      "val Loss: 20.9211 Acc: 0.2452\n",
      "Epoch time  19.044712781906128\n",
      "\n",
      "train Loss: 12.8604 Acc: 0.3390\n",
      "val Loss: 19.4723 Acc: 0.2463\n",
      "Epoch time  18.245521306991577\n",
      "\n",
      "train Loss: 12.7018 Acc: 0.3396\n",
      "val Loss: 20.7341 Acc: 0.2533\n",
      "Epoch time  18.380958795547485\n",
      "\n",
      "train Loss: 12.7307 Acc: 0.3394\n",
      "val Loss: 18.8500 Acc: 0.2570\n",
      "Epoch time  18.496174812316895\n",
      "\n",
      "train Loss: 12.8568 Acc: 0.3417\n",
      "val Loss: 19.7042 Acc: 0.2504\n",
      "Epoch time  18.24916434288025\n",
      "\n",
      "train Loss: 12.6636 Acc: 0.3449\n",
      "val Loss: 18.7443 Acc: 0.2600\n",
      "Epoch time  18.721580028533936\n",
      "\n",
      "train Loss: 12.6976 Acc: 0.3452\n",
      "val Loss: 19.8648 Acc: 0.2495\n",
      "Epoch time  18.54782462120056\n",
      "\n",
      "train Loss: 12.6262 Acc: 0.3445\n",
      "val Loss: 19.3743 Acc: 0.2564\n",
      "Epoch time  18.895668745040894\n",
      "\n",
      "train Loss: 12.6577 Acc: 0.3451\n",
      "val Loss: 19.8468 Acc: 0.2489\n",
      "Epoch time  18.226473331451416\n",
      "\n",
      "train Loss: 12.8322 Acc: 0.3422\n",
      "val Loss: 18.8089 Acc: 0.2530\n",
      "Epoch time  18.200000047683716\n",
      "\n",
      "train Loss: 12.8065 Acc: 0.3433\n",
      "val Loss: 18.7889 Acc: 0.2570\n",
      "Epoch time  18.594185829162598\n",
      "\n",
      "train Loss: 12.8265 Acc: 0.3414\n",
      "val Loss: 20.6364 Acc: 0.2526\n",
      "Epoch time  18.247527360916138\n",
      "\n",
      "train Loss: 12.9073 Acc: 0.3421\n",
      "val Loss: 19.8451 Acc: 0.2527\n",
      "Epoch time  18.78353762626648\n",
      "\n",
      "train Loss: 12.7418 Acc: 0.3437\n",
      "val Loss: 18.2947 Acc: 0.2527\n",
      "Epoch time  18.217689990997314\n",
      "\n",
      "Training complete in 23m 8s\n",
      "Best val Acc: 0.260000\n"
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 100)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=10000, gamma=0.1)\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6SdyU9oXwKb",
    "outputId": "fa904279-97e6-47ad-fbda-7554067839b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 3.5442 Acc: 0.2114\n",
      "val Loss: 3.3559 Acc: 0.2617\n",
      "Epoch time  19.403121948242188\n",
      "\n",
      "train Loss: 3.1341 Acc: 0.2759\n",
      "val Loss: 3.4352 Acc: 0.2708\n",
      "Epoch time  18.895296573638916\n",
      "\n",
      "train Loss: 2.9919 Acc: 0.3022\n",
      "val Loss: 3.2662 Acc: 0.2799\n",
      "Epoch time  19.361782789230347\n",
      "\n",
      "train Loss: 2.9110 Acc: 0.3138\n",
      "val Loss: 3.2119 Acc: 0.2892\n",
      "Epoch time  18.928969860076904\n",
      "\n",
      "train Loss: 2.8296 Acc: 0.3280\n",
      "val Loss: 3.3056 Acc: 0.3007\n",
      "Epoch time  19.208881616592407\n",
      "\n",
      "train Loss: 2.7768 Acc: 0.3367\n",
      "val Loss: 3.2525 Acc: 0.2963\n",
      "Epoch time  18.850311994552612\n",
      "\n",
      "train Loss: 2.7262 Acc: 0.3470\n",
      "val Loss: 3.3176 Acc: 0.2941\n",
      "Epoch time  19.041600942611694\n",
      "\n",
      "train Loss: 2.6983 Acc: 0.3518\n",
      "val Loss: 3.1771 Acc: 0.3066\n",
      "Epoch time  19.048458337783813\n",
      "\n",
      "train Loss: 2.6522 Acc: 0.3611\n",
      "val Loss: 3.2359 Acc: 0.3032\n",
      "Epoch time  18.52427339553833\n",
      "\n",
      "train Loss: 2.6277 Acc: 0.3655\n",
      "val Loss: 3.1331 Acc: 0.3037\n",
      "Epoch time  18.40889811515808\n",
      "\n",
      "train Loss: 2.5934 Acc: 0.3689\n",
      "val Loss: 3.1166 Acc: 0.3104\n",
      "Epoch time  18.87989377975464\n",
      "\n",
      "train Loss: 2.5621 Acc: 0.3744\n",
      "val Loss: 3.2372 Acc: 0.3061\n",
      "Epoch time  18.623754501342773\n",
      "\n",
      "train Loss: 2.5564 Acc: 0.3780\n",
      "val Loss: 3.1887 Acc: 0.3115\n",
      "Epoch time  18.618058443069458\n",
      "\n",
      "train Loss: 2.5302 Acc: 0.3841\n",
      "val Loss: 3.2374 Acc: 0.3022\n",
      "Epoch time  18.632251501083374\n",
      "\n",
      "train Loss: 2.5187 Acc: 0.3851\n",
      "val Loss: 3.2552 Acc: 0.3022\n",
      "Epoch time  19.01533007621765\n",
      "\n",
      "train Loss: 2.4954 Acc: 0.3893\n",
      "val Loss: 3.1502 Acc: 0.3099\n",
      "Epoch time  18.831323623657227\n",
      "\n",
      "train Loss: 2.4836 Acc: 0.3928\n",
      "val Loss: 3.2789 Acc: 0.3135\n",
      "Epoch time  18.7955961227417\n",
      "\n",
      "train Loss: 2.4639 Acc: 0.3963\n",
      "val Loss: 3.3374 Acc: 0.3035\n",
      "Epoch time  18.36560559272766\n",
      "\n",
      "train Loss: 2.4742 Acc: 0.3931\n",
      "val Loss: 3.3446 Acc: 0.3031\n",
      "Epoch time  18.972751140594482\n",
      "\n",
      "train Loss: 2.4531 Acc: 0.3986\n",
      "val Loss: 3.2689 Acc: 0.3044\n",
      "Epoch time  18.28124237060547\n",
      "\n",
      "train Loss: 2.4396 Acc: 0.4020\n",
      "val Loss: 3.2329 Acc: 0.3081\n",
      "Epoch time  18.6227285861969\n",
      "\n",
      "train Loss: 2.4406 Acc: 0.3991\n",
      "val Loss: 3.2348 Acc: 0.3159\n",
      "Epoch time  18.66690754890442\n",
      "\n",
      "train Loss: 2.4265 Acc: 0.4040\n",
      "val Loss: 3.2271 Acc: 0.3143\n",
      "Epoch time  18.24847936630249\n",
      "\n",
      "train Loss: 2.4033 Acc: 0.4064\n",
      "val Loss: 3.1489 Acc: 0.3175\n",
      "Epoch time  18.82951259613037\n",
      "\n",
      "train Loss: 2.4092 Acc: 0.4068\n",
      "val Loss: 3.3193 Acc: 0.3084\n",
      "Epoch time  18.524781703948975\n",
      "\n",
      "train Loss: 2.4084 Acc: 0.4079\n",
      "val Loss: 3.3101 Acc: 0.3067\n",
      "Epoch time  18.518829107284546\n",
      "\n",
      "train Loss: 2.3916 Acc: 0.4125\n",
      "val Loss: 3.1954 Acc: 0.3110\n",
      "Epoch time  18.404813051223755\n",
      "\n",
      "train Loss: 2.3961 Acc: 0.4117\n",
      "val Loss: 3.2804 Acc: 0.3095\n",
      "Epoch time  18.16751790046692\n",
      "\n",
      "train Loss: 2.3834 Acc: 0.4127\n",
      "val Loss: 3.2430 Acc: 0.3153\n",
      "Epoch time  18.6141836643219\n",
      "\n",
      "train Loss: 2.3736 Acc: 0.4155\n",
      "val Loss: 3.4780 Acc: 0.3069\n",
      "Epoch time  18.60235333442688\n",
      "\n",
      "train Loss: 2.3818 Acc: 0.4128\n",
      "val Loss: 3.2801 Acc: 0.3106\n",
      "Epoch time  18.390260934829712\n",
      "\n",
      "train Loss: 2.3818 Acc: 0.4139\n",
      "val Loss: 3.4185 Acc: 0.3049\n",
      "Epoch time  18.763928174972534\n",
      "\n",
      "train Loss: 2.3689 Acc: 0.4171\n",
      "val Loss: 3.3525 Acc: 0.3060\n",
      "Epoch time  19.003846883773804\n",
      "\n",
      "train Loss: 2.3676 Acc: 0.4152\n",
      "val Loss: 3.3327 Acc: 0.3085\n",
      "Epoch time  18.619906425476074\n",
      "\n",
      "train Loss: 2.3617 Acc: 0.4204\n",
      "val Loss: 3.3232 Acc: 0.3079\n",
      "Epoch time  18.17520523071289\n",
      "\n",
      "train Loss: 2.3540 Acc: 0.4204\n",
      "val Loss: 3.4608 Acc: 0.3020\n",
      "Epoch time  18.2437162399292\n",
      "\n",
      "train Loss: 2.3448 Acc: 0.4226\n",
      "val Loss: 3.2310 Acc: 0.3159\n",
      "Epoch time  19.141518115997314\n",
      "\n",
      "train Loss: 2.3594 Acc: 0.4192\n",
      "val Loss: 3.3657 Acc: 0.3056\n",
      "Epoch time  18.504662036895752\n",
      "\n",
      "train Loss: 2.3631 Acc: 0.4209\n",
      "val Loss: 3.3331 Acc: 0.3050\n",
      "Epoch time  18.847368955612183\n",
      "\n",
      "train Loss: 2.3484 Acc: 0.4207\n",
      "val Loss: 3.4411 Acc: 0.3085\n",
      "Epoch time  18.6252019405365\n",
      "\n",
      "train Loss: 2.3476 Acc: 0.4225\n",
      "val Loss: 3.4440 Acc: 0.3094\n",
      "Epoch time  18.4102463722229\n",
      "\n",
      "train Loss: 2.3406 Acc: 0.4228\n",
      "val Loss: 3.4599 Acc: 0.2984\n",
      "Epoch time  18.949217081069946\n",
      "\n",
      "train Loss: 2.3326 Acc: 0.4250\n",
      "val Loss: 3.3800 Acc: 0.3065\n",
      "Epoch time  17.92943525314331\n",
      "\n",
      "train Loss: 2.3373 Acc: 0.4273\n",
      "val Loss: 3.4234 Acc: 0.3109\n",
      "Epoch time  18.14945673942566\n",
      "\n",
      "train Loss: 2.3483 Acc: 0.4236\n",
      "val Loss: 3.4250 Acc: 0.3127\n",
      "Epoch time  18.59183382987976\n",
      "\n",
      "train Loss: 2.3452 Acc: 0.4253\n",
      "val Loss: 3.3715 Acc: 0.3066\n",
      "Epoch time  18.62247633934021\n",
      "\n",
      "train Loss: 2.3275 Acc: 0.4275\n",
      "val Loss: 3.4606 Acc: 0.3033\n",
      "Epoch time  18.950173139572144\n",
      "\n",
      "train Loss: 2.3305 Acc: 0.4273\n",
      "val Loss: 3.4212 Acc: 0.3136\n",
      "Epoch time  18.578193426132202\n",
      "\n",
      "train Loss: 2.3288 Acc: 0.4259\n",
      "val Loss: 3.5612 Acc: 0.3058\n",
      "Epoch time  18.40757417678833\n",
      "\n",
      "train Loss: 2.3154 Acc: 0.4300\n",
      "val Loss: 3.4103 Acc: 0.3092\n",
      "Epoch time  18.22861409187317\n",
      "\n",
      "Training complete in 15m 33s\n",
      "Best val Acc: 0.317500\n"
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 100)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=10000, gamma=0.1)\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-vQF4UZXxhH",
    "outputId": "0b295444-affc-444f-8a3b-e0bdd43e1cc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 3.9935 Acc: 0.1467\n",
      "val Loss: 3.5322 Acc: 0.2287\n",
      "Epoch time  19.959953546524048\n",
      "\n",
      "train Loss: 3.3874 Acc: 0.2439\n",
      "val Loss: 3.2941 Acc: 0.2613\n",
      "Epoch time  20.366072416305542\n",
      "\n",
      "train Loss: 3.1817 Acc: 0.2698\n",
      "val Loss: 3.2008 Acc: 0.2737\n",
      "Epoch time  19.445040225982666\n",
      "\n",
      "train Loss: 3.0662 Acc: 0.2885\n",
      "val Loss: 3.0898 Acc: 0.2850\n",
      "Epoch time  19.50485372543335\n",
      "\n",
      "train Loss: 2.9790 Acc: 0.2996\n",
      "val Loss: 3.0586 Acc: 0.2926\n",
      "Epoch time  19.982303857803345\n",
      "\n",
      "train Loss: 2.9200 Acc: 0.3099\n",
      "val Loss: 3.0265 Acc: 0.2957\n",
      "Epoch time  20.21051859855652\n",
      "\n",
      "train Loss: 2.8776 Acc: 0.3145\n",
      "val Loss: 2.9862 Acc: 0.3019\n",
      "Epoch time  19.518339157104492\n",
      "\n",
      "train Loss: 2.8393 Acc: 0.3210\n",
      "val Loss: 2.9958 Acc: 0.3021\n",
      "Epoch time  19.458975315093994\n",
      "\n",
      "train Loss: 2.8078 Acc: 0.3263\n",
      "val Loss: 2.9452 Acc: 0.3074\n",
      "Epoch time  19.709296941757202\n",
      "\n",
      "train Loss: 2.7755 Acc: 0.3326\n",
      "val Loss: 2.9599 Acc: 0.3092\n",
      "Epoch time  19.517035484313965\n",
      "\n",
      "train Loss: 2.7530 Acc: 0.3359\n",
      "val Loss: 2.9319 Acc: 0.3120\n",
      "Epoch time  19.618958950042725\n",
      "\n",
      "train Loss: 2.7283 Acc: 0.3426\n",
      "val Loss: 2.9482 Acc: 0.3127\n",
      "Epoch time  19.05287766456604\n",
      "\n",
      "train Loss: 2.7117 Acc: 0.3416\n",
      "val Loss: 2.9570 Acc: 0.3041\n",
      "Epoch time  19.66894221305847\n",
      "\n",
      "train Loss: 2.6852 Acc: 0.3486\n",
      "val Loss: 2.8926 Acc: 0.3185\n",
      "Epoch time  19.615896463394165\n",
      "\n",
      "train Loss: 2.6665 Acc: 0.3511\n",
      "val Loss: 2.9115 Acc: 0.3174\n",
      "Epoch time  19.483325242996216\n",
      "\n",
      "train Loss: 2.6488 Acc: 0.3547\n",
      "val Loss: 2.9128 Acc: 0.3157\n",
      "Epoch time  19.270423412322998\n",
      "\n",
      "train Loss: 2.6314 Acc: 0.3589\n",
      "val Loss: 2.9433 Acc: 0.3139\n",
      "Epoch time  19.33624529838562\n",
      "\n",
      "train Loss: 2.6134 Acc: 0.3595\n",
      "val Loss: 2.9529 Acc: 0.3196\n",
      "Epoch time  19.628254652023315\n",
      "\n",
      "train Loss: 2.6022 Acc: 0.3626\n",
      "val Loss: 2.9105 Acc: 0.3196\n",
      "Epoch time  19.042804479599\n",
      "\n",
      "train Loss: 2.5950 Acc: 0.3618\n",
      "val Loss: 2.8631 Acc: 0.3269\n",
      "Epoch time  19.532416343688965\n",
      "\n",
      "train Loss: 2.5731 Acc: 0.3682\n",
      "val Loss: 2.8690 Acc: 0.3235\n",
      "Epoch time  19.56715202331543\n",
      "\n",
      "train Loss: 2.5677 Acc: 0.3689\n",
      "val Loss: 2.8923 Acc: 0.3231\n",
      "Epoch time  19.636541843414307\n",
      "\n",
      "train Loss: 2.5519 Acc: 0.3715\n",
      "val Loss: 2.8343 Acc: 0.3289\n",
      "Epoch time  19.426271677017212\n",
      "\n",
      "train Loss: 2.5375 Acc: 0.3755\n",
      "val Loss: 2.8711 Acc: 0.3204\n",
      "Epoch time  19.47784161567688\n",
      "\n",
      "train Loss: 2.5239 Acc: 0.3768\n",
      "val Loss: 2.8793 Acc: 0.3198\n",
      "Epoch time  19.660542249679565\n",
      "\n",
      "train Loss: 2.5177 Acc: 0.3781\n",
      "val Loss: 2.8541 Acc: 0.3226\n",
      "Epoch time  19.75941562652588\n",
      "\n",
      "train Loss: 2.5128 Acc: 0.3812\n",
      "val Loss: 2.9201 Acc: 0.3183\n",
      "Epoch time  19.426102876663208\n",
      "\n",
      "train Loss: 2.5022 Acc: 0.3820\n",
      "val Loss: 2.8427 Acc: 0.3265\n",
      "Epoch time  19.935295343399048\n",
      "\n",
      "train Loss: 2.4928 Acc: 0.3860\n",
      "val Loss: 2.8338 Acc: 0.3273\n",
      "Epoch time  19.68164610862732\n",
      "\n",
      "train Loss: 2.4810 Acc: 0.3861\n",
      "val Loss: 2.8500 Acc: 0.3264\n",
      "Epoch time  19.350202560424805\n",
      "\n",
      "train Loss: 2.4708 Acc: 0.3864\n",
      "val Loss: 2.8222 Acc: 0.3295\n",
      "Epoch time  19.336381435394287\n",
      "\n",
      "train Loss: 2.4684 Acc: 0.3890\n",
      "val Loss: 2.9043 Acc: 0.3247\n",
      "Epoch time  19.626673698425293\n",
      "\n",
      "train Loss: 2.4520 Acc: 0.3918\n",
      "val Loss: 2.8515 Acc: 0.3240\n",
      "Epoch time  19.622793197631836\n",
      "\n",
      "train Loss: 2.4515 Acc: 0.3918\n",
      "val Loss: 2.8746 Acc: 0.3278\n",
      "Epoch time  19.077847003936768\n",
      "\n",
      "train Loss: 2.4325 Acc: 0.3938\n",
      "val Loss: 2.8638 Acc: 0.3322\n",
      "Epoch time  19.698068141937256\n",
      "\n",
      "train Loss: 2.4399 Acc: 0.3937\n",
      "val Loss: 2.8519 Acc: 0.3321\n",
      "Epoch time  19.128153324127197\n",
      "\n",
      "train Loss: 2.4281 Acc: 0.3969\n",
      "val Loss: 2.8298 Acc: 0.3298\n",
      "Epoch time  19.958778619766235\n",
      "\n",
      "train Loss: 2.4167 Acc: 0.3981\n",
      "val Loss: 2.8240 Acc: 0.3332\n",
      "Epoch time  19.65030550956726\n",
      "\n",
      "train Loss: 2.4104 Acc: 0.3988\n",
      "val Loss: 2.8124 Acc: 0.3344\n",
      "Epoch time  19.456470012664795\n",
      "\n",
      "train Loss: 2.4032 Acc: 0.4025\n",
      "val Loss: 2.7912 Acc: 0.3352\n",
      "Epoch time  19.119669437408447\n",
      "\n",
      "train Loss: 2.3969 Acc: 0.4028\n",
      "val Loss: 2.8322 Acc: 0.3304\n",
      "Epoch time  18.86163902282715\n",
      "\n",
      "train Loss: 2.3946 Acc: 0.4017\n",
      "val Loss: 2.8289 Acc: 0.3293\n",
      "Epoch time  19.548084259033203\n",
      "\n",
      "train Loss: 2.3833 Acc: 0.4072\n",
      "val Loss: 2.7751 Acc: 0.3316\n",
      "Epoch time  19.38201332092285\n",
      "\n",
      "train Loss: 2.3790 Acc: 0.4015\n",
      "val Loss: 2.7780 Acc: 0.3347\n",
      "Epoch time  18.83073139190674\n",
      "\n",
      "train Loss: 2.3702 Acc: 0.4085\n",
      "val Loss: 2.8138 Acc: 0.3345\n",
      "Epoch time  18.913022756576538\n",
      "\n",
      "train Loss: 2.3622 Acc: 0.4093\n",
      "val Loss: 2.7819 Acc: 0.3343\n",
      "Epoch time  18.922974348068237\n",
      "\n",
      "train Loss: 2.3659 Acc: 0.4080\n",
      "val Loss: 2.7631 Acc: 0.3355\n",
      "Epoch time  18.783432960510254\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet50(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 100)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=10000, gamma=0.1)\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRBDrzZfmbcX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKPBCgTzmbcX"
   },
   "source": [
    "2.(a)\n",
    "The 0.001 learning rate gives the best accuracy on the target dataset = 0.3355\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxG6HHjUmbcY"
   },
   "source": [
    "2.(b) The finetuning approach gave better results than the feature extraction approach. Among all the models, the finetuning model with a small learning rate ranging from 0.0001 to 0.000001 gave the best accuracy. This is because we expect the pretrained weights to be relatively good estimates and high learning rates may distort them too quickly.\n",
    "The feature extractor model may have given low accuracies because the ImageNet dataset which it is trained on might not be that similar to the CIFAR-100 dataset. So the feature extractioin is not as good as expected. As the resnet model has batch normalization layers, which might be input dependent, changing the input distribution drastically may affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qsqcaGJ-mbcY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qf3f5R2pmbcY"
   },
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIQ8ASqgmssu"
   },
   "source": [
    "1. In the paper which uses weakly supervised learning, they use noisy labels(hashtags) to improve the accuracy.\n",
    "\n",
    "Whereas the semi-supervised learninng paper uses a teacher-student model in which a teacher model is trained using labeled data and is used to label unseen unlabeled data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pu95bZ6mmtEo"
   },
   "source": [
    "2.(a) Yes, the models trained using hashtags are robust again noise labels. In the paper they pre-trained a ResNeXt model with 1B images and 17k labels where p% of the labels were randomly replaced with noise. p = 10% decreased the top-1 accuracy on ImageNet by only around 1%, and p = 25% decreased accuracy by about 2%.\n",
    "\n",
    "\n",
    "(b) Hashtags follow Zipfian distribution and it may reduce the impact of some of the classes on the overall training loss. Resampling the hashtag distribution ensures that all classes are included in training. Resampling of the hashtag distribution is important in order to obtain good transfer to ImageNet image-classification tasks. Using uniform or square-root sampling leads to an accuracy improvement of 5 to 6% irrespective of the number of ImageNet classes in the transfer task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DQSj-qdmzso"
   },
   "source": [
    "3.a) The goal is to improve performance using unlabeled data. This paper suggest using two models, teacher and student so that one model can benefit from the other. The teacher model is trained using labeled data. The unlabeled data is then passed through the teacher model and top-K labels from each target variable are selected from this to create a new labeled dataset. The student model is then trained on this new dataset after which it is finetuned on the previvous labeled dataset. So the student model uses the labels generated from the teacher model to train itself.\n",
    "\n",
    "Distillation is a procedure used to compress a large model into a smaller one. In distillation the teacher model makes prediction on unlabelled data, and the inferred labels are used to train the student in a supervised fashion which is very similar to the model suggested in the paper. Therefore the teacher-student model is a type of distillation technique.\n",
    "\n",
    "The teacher and student model is needed since the teacher model selects the top-K images in the unlabeled dataset and this dataset allows us generate a new training set. Since the student model is always 'smaller' than the teacher model, it is a distillation technique.\n",
    "\n",
    "b) K is the number of examples that are selected from the unlabeled dataset U for each target label. P corresponds to the number of relevant classes of an image. The reason for choosing P > 1 is that it is difficult to identify\n",
    "accurately under-represented objects, or some may be cut-off by more prominent co-occurring objects.\n",
    "\n",
    "c) The new labeled dataset is created by selecting the top-K images from each class from the predictions of the teacher model.\n",
    "Yes an image from this dataset can belong to more than one class. As P>1, if the scores of the image for more than one class lies in the top K scores of those respectiive classes then it will belong to all such classes.\n",
    "\n",
    "d) Increasing K initially increases the amount of data present for the student model and thus causes the accuracy to increase. But upon increasing K further, we observe a drop ini accuracy because this causes incorrect results from the output of the teacher model to be added to the new labeled data. As this increases the noise in the labels of the new dataset the accuracy decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQZyH0ZampBI"
   },
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSDxe-RtnIgl"
   },
   "source": [
    "1. Achieving peak FLOPS requires customized libraries with intimate knowledge of the underlying hardware. Even specially tuned libraries may fall short of peak execution by as much as 40%. \n",
    "Instead of trying to measure and capture every source of inefficiency in every learning framework, the paper suggests taking a small number of representative deep learning workloads which contain convolutions, pooling, dropout, and fully connected layers and run them for a short time on a single GPU. Given observed total throughput and estimated total throughput on this benchmark, fit a scaling constant to estimate a platform percent of peak (PPP) parameter which captures the average relative inefficiency of the platform compared to peak FLOPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjYVCW42ngnt"
   },
   "source": [
    "2. The VGG19 model has a conv3-256 and 2 conv3-512 layers in addition to the VGG16 moodel. Therefore additional FLOPs = 4,161,798,144 = 4162M\n",
    "\n",
    "The distribution of FLOPs for the VGG16 model is as follows-\n",
    "\n",
    "CONV:15360M,\n",
    "POOL:6M,\n",
    "ReLU:14M,\n",
    "FC:124M,\n",
    "\n",
    "Upon adding the additional FLOPs,\n",
    "\n",
    "\n",
    "The CONV layers FLOPs of the VGG19 network: 19522M\n",
    "\n",
    "And the total FLOPs of the VGG19 network is: 15503M + 4162M = 19,665M\n",
    "\n",
    "So the fraction of the total FLOPs attributed by convolution layers 0.9932."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPyKUZGfnMLC"
   },
   "source": [
    "3. The measured time and sum of layerwise timing for forward pass did not match on GPUs. This is because CUDA allows asynchronous programming. Before the time is measured, an API is called to ensure that all cores have finished their tasks. This synchronization before measuring time on the GPUs results in an extra overhead. Thererefore, the sum of layerwise timing on GPUs is longer\n",
    "than a full forward pass.\n",
    "\n",
    "In a full forward pass, timing is only recorded at the last layer. Therefore, a core may be assigned with the computation of following layers and thus it can continuously perform the computation without synchronization. For example,\n",
    "after finishing the multiply-add operations for the matrix multiplication at a CONV layer, a core can continue to calculate the max function of next ReLU layer on the output of multiply-add operations. If layerwise timing is recorded, all cores have to wait until all multiply-add operations of the CONV layer have been completed.\n",
    "\n",
    "To mitigate the overhead, they keep GPUs iteratively running the process in a way that GPU cores can continuously perform multiply-add operations without synchronization, before recording the end time. Then, the measurement overhead is amortized over all the iterations, giving accurate timing estimates. When the number of iterations is large the measurement overhead becomes insignificant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJE356cgnQXG"
   },
   "source": [
    "4. NVidia Tesla K80: double PPP = 1.87 Tflops.\n",
    "Forward pass on VGG requires 15503M FLOPs; as such, one forward pass on a K80 would take (15503 x 10^6) / (1.87 x 10^12) = 0.00829037433s so throughput is 120 images/sec.\n",
    "\n",
    "GoogLeNet: \n",
    "Inference time: (1606 x 10^6) / (1.87 x 10^12) = 0.0008588s/image. Throughoutput: 1164 images/sec.\n",
    "\n",
    "ResNet: \n",
    "Inference time: (3922 x 10^6) / (1.87 x 10^12) = 0.0020973s/img. \n",
    "Throughput: 476 images/sec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMkIa1ljnlEY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL_hw4_p1final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06d2e04735464f83b6f8701b70eb59e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2791c6950e8a46aaab67f6a29852a15b",
      "placeholder": "",
      "style": "IPY_MODEL_13b5eeb83e104c86bc30b189bb62b126",
      "value": " 169001984/? [00:04&lt;00:00, 35279603.90it/s]"
     }
    },
    "13b5eeb83e104c86bc30b189bb62b126": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2791c6950e8a46aaab67f6a29852a15b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27c4809878264ddca62e70e6acf4e907": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "357252f5ea7a489dbbb6cd1439bf7412": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "929dde6e586c4c96996398be4261549a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_357252f5ea7a489dbbb6cd1439bf7412",
      "placeholder": "",
      "style": "IPY_MODEL_27c4809878264ddca62e70e6acf4e907",
      "value": ""
     }
    },
    "9373f1b3659a49f79d1de6949ac01105": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_929dde6e586c4c96996398be4261549a",
       "IPY_MODEL_f066ba75d2874803a77a9b4885d0d33c",
       "IPY_MODEL_06d2e04735464f83b6f8701b70eb59e4"
      ],
      "layout": "IPY_MODEL_bdcd2bd4c9224325819a3d22aa27adf8"
     }
    },
    "bdcd2bd4c9224325819a3d22aa27adf8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed442ba6c9414e66abb543363152528f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f066ba75d2874803a77a9b4885d0d33c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f67eaa867b584de79feb8f272dcacbd2",
      "max": 169001437,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ed442ba6c9414e66abb543363152528f",
      "value": 169001437
     }
    },
    "f67eaa867b584de79feb8f272dcacbd2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
